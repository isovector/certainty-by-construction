# Functions, Big and Small

```agda
module functions where

open import Level using (Level)
```

Computer science is chocked full of data structures. A great many come from the
official pantheon---things like binary search trees, hash maps, stacks, graphs,
and heaps. But, dwarfing all of these, there exists orders of magnitude more
data structures in the arcane vault, from the
passingly-familiar-but-unlikely-to-have-implemented *rope* to the obscure *Judy
array.* With so many options to choose from, how can we even hope to make an
informed choice?

The reason there exist many more data structures than any practitioner can
possibly know about is that most data structures are minor tweaks of other
well-known structures. For example, the UB-tree is a variation on the B+ tree,
which itself is a B-tree that maintains a particular invariant, while a B-tree
is a generalization of the binary search tree (BST henceforth). Unrolling the
lineage here shows us that whatever the UB-tree is, it's probably a BST that has
more desirable computational properties for certain shapes of data.

As Donald Knuth said, "premature optimization is the root of all evil." For the
vast majority of tasks, you can start with (and subsequently get away with) a
BST, upgrading to the more complex UB-tree in the future only if it turns out to
be mission critical. This is a well-understood idea in the modern age.

However, most programmers coming to Agda make an error in the spirit of the
Co-Blub paradox. After years of honing their taste and cutting their teeth on
picking the simplest data structure for the job, they come to Agda and
immediately fall down the rabbit-hole of long, arduous proofs. As I have gotten
older and more experienced, my guiding philosophy for writing software has
become *if it feels too hard, it probably is.*

As it happens, your choice of representation matters much more in Agda than it
does in most programming languages. That arises from the fact that your proofs
will inevitably trace the same grooves as the implementations they are proofs
*about.* In other words, the proof follows the implementation. It's not hard to
imagine that a complicated implementation will warrant a complicated proof.


## Matrices

Let's work through an example together, to get a feel for just how important a
representation can be. Our object of investigation will be *matrices*---that is,
rectangular arrays of numbers. Matrices are often used in computational
geometry, including 3D graphics, and are the back-bone of modern machine
learning techniques. As an exercise in honing our
translating-mathematics-to-Agda chops, let's take a look at the definition of a
matrix.

Matrix
:   A rectangular array of numbers.

Matrices have a predefined height and width, often referred to as $m$ and $n$
respectively, and given in that order. For example, the following is a 3x2
matrix:

```text
1   1
5  -42
0  2.5
```

Note that the numbers inside a matrix are rather flexible. Depending on the
circumstances, we might prefer them to be naturals, while in others we might
want reals, or even functions. In order to avoid the complexities here, we will
simply parameterize the our module over the type of numbers, and postulate any
properties of those numbers as the need occurs. Let's call this number type
parameter `ğ”¸`:

```agda
module matrix-induction {ğ”¸ : Set} where
```


### The Row-Major Representation

Returning to the problem of modeling matrices in Agda, note that we don't have
any good, inductive primitives for two-dimensional data, I think most
programmers would thus come up with the next best thing: the "vector of vectors"
model---indeed, it's what I first thought of.

```agda
  open import Data.Product
    as Î£
    using (_Ã—_; _,_)
  open import Data.Nat
    using (â„•; zero; suc)
  open import Data.Vec
    using (Vec; []; _âˆ·_)

  Matrix : â„• â†’ â„• â†’ Set
  Matrix m n = Vec (Vec ğ”¸ n) m

  private variable
    m n p : â„•
```

This representation is known as the "row-major order" of matrices, that is, the
rows have contiguous data, while the columns do not. There are immediate
repercussions here. For example, let's implement the function `top/rest` which
separates the first row from the rest of the matrix:

```agda
  top/rest
      : Matrix (suc m) n
      â†’ Vec ğ”¸ n Ã— Matrix m n
  top/rest (x âˆ· xs) = x , xs
```

Compare `top/rest` to the analogous function that pulls the leftmost column off
of a matrix:

```agda
  left/rest
      : Matrix m (suc n)
      â†’ Vec ğ”¸ m Ã— Matrix m n
  left/rest [] = [] , []
  left/rest ((x âˆ· v) âˆ· m)
    = Î£.map (x âˆ·_) (v âˆ·_) (left/rest m)
```

The dramatic difference in complexity between these two analogous functions is
telling. Clearly, row-major order significantly privileges working with rows
over working with columns.

Nevertheless, we can continue by implementing a few special matrices of note.
First is the zero-matrix, which is the matrix that is full only of zeroes. Note
that we will also need to postulate the existence of `0# : ğ”¸`.

```agda
  postulate 0# : ğ”¸

  open Data.Vec
    using (replicate)

  0â‚˜ : Matrix m n
  0â‚˜ = replicate (replicate 0#)
```

Two matrices of the same dimensions support a kind of addition, given by adding
the respective cells in each of the two columns. That is:

$$
\begin{bmatrix}
a & b & c\\
d & e & f
\end{bmatrix}
+
\begin{bmatrix}
x & y & z\\
t & u & v
\end{bmatrix}
=
\begin{bmatrix}
a + x & b + y & c + z\\
d + t & e + u & f + v
\end{bmatrix}
$$

We can implement this operation over matrices by positing the existence of an
addition over `ğ”¸`, as well as some common-sense identity laws:

```agda
  open import Relation.Binary.PropositionalEquality

  postulate
    _+_ : ğ”¸ â†’ ğ”¸ â†’ ğ”¸
    +-identityË¡ : âˆ€ x â†’ 0# + x â‰¡ x
    +-identityÊ³ : âˆ€ x â†’ x + 0# â‰¡ x

  open Data.Vec
    using (zipWith)
```

Addition of matrices doesn't present us any problems, as pointwise operations
don't need to distinguish between rows and columns. Thus, we can zip the rows
together, zip the corresponding cells together, and add each pair:

```agda
  _+â‚˜_ : Matrix m n â†’ Matrix m n â†’ Matrix m n
  x +â‚˜ y = zipWith (zipWith _+_) x y
```

Let's now prove the trivial fact that `0â‚˜` is a left identity for `+â‚˜`:

```agda
  +â‚˜-identityË¡ : (x : Matrix m n) â†’ 0â‚˜ +â‚˜ x â‰¡ x
```

We can begin, as always, with induction on our argument. The first case, in
which `m â‰¡ 0`, is easy:

```agda
  +â‚˜-identityË¡ [] = refl
```

The case that `n â‰¡ 0` is also easy, although slightly more work, as our
row-major order would suggest:

```agda
  +â‚˜-identityË¡ ([] âˆ· rs)
    rewrite +â‚˜-identityË¡ rs
      = refl
```

We're now left with the induction case. After some obvious rewriting to
eliminate the `0# +_` and the row-recursive case, we're left here:

```agda
  +â‚˜-identityË¡ ((c âˆ· cs) âˆ· rs)
    rewrite +-identityË¡ c
    rewrite +â‚˜-identityË¡ rs
```

with the goal

```goal
  (c âˆ· zipWith _+_ (replicate 0#) cs) âˆ· rs
â‰¡
  (c âˆ· cs) âˆ· rs
```

and it's unclear how to move forwards. It would be nice if our induction just
worked, but, unfortunately, it doesn't. Crossing our fingers that this is not a
serious problem, we can write a little lemma to solve the goal for us:

```agda
      = cong (Î» Ï† â†’ (c âˆ· Ï†) âˆ· rs) (lemma cs)

    where
      lemma
          : âˆ€ {m} (cs : Vec ğ”¸ m)
          â†’ zipWith _+_ (replicate 0#) cs â‰¡ cs
      lemma [] = refl
      lemma (c âˆ· cs)
        rewrite +-identityË¡ c
        rewrite lemma cs
          = refl
```

It's not the tidiest proof in the world, but it certainly gets the job done.
However, we should be wary here; this is our second function in which dealing
with the columns was clunkier than the same operation over the rows.

Addition, however, is not the primary task for which programmers and
mathematicians use matrices. No, the more interesting operation is *matrix
multiplication.* Matrix multiplication, unlike your everyday multiplication, has
a stronger type, and requires our two matrices to have an equal dimension
between them. That is, the matrix on the left must have the same width as the
height of the matrix on the right. That is, given `a : Matrix m n` and `b :
Matrix n p`, we can write the operation `a *â‚˜ b` in symbols as:

$$
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,n}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \cdots & a_{m,n}
\end{bmatrix}
\times
\begin{bmatrix}
b_{1,1} & b_{1,2} & \cdots & b_{1,p}\\
b_{2,1} & b_{2,2} & \cdots & b_{2,p}\\
\vdots & \vdots & \ddots & \vdots \\
b_{n,1} & b_{n,2} & \cdots & b_{n,p}
\end{bmatrix}
$$

with the result being `c : Matrix m p`, where each cell is given by the formula:

$$
c_{i,j} = \sum_{k = 1}^{n} a_{i,k} \times b_{k, j}
$$

Said another way, the product matrix resulting from a multiplication pairs the
rows of the first matrix with the columns of the second, adding each cell up
pointwise.

If this is your first time seeing matrix multiplication (or even if it isn't,)
it might be unclear what the *intuition* behind matrix multiplication is. Why
does it exist, what does it do, and why should we care about it? We will return
to this question in a moment, but for the time being, resign ourselves to
implementing it in our row-major matrix representation.

We will implement matrix multiplication in two steps; first, by computing the
*outer-product*, which is the analogous operation on vectors (matrices with one
dimension set to 1.) The outer product of two vectors is a matrix using the
length of the first as its height, and the length of the second as its width. In
symbols, the result of:

$$
\begin{bmatrix}
a_{1} \\
a_{2} \\
\vdots \
a_{m}
\end{bmatrix}
\otimes
\begin{bmatrix}
b_{1} \\
b_{2} \\
\vdots \
b_{n}
\end{bmatrix}
$$

is a matrix:

$$
\begin{bmatrix}
a_{1}\times b_{1} & a_{1}\times b_{2} & \cdots & a_{1}\times b_{n}\\
a_{2}\times b_{1} & a_{2}\times b_{2} & \cdots & a_{2}\times b_{n}\\
\vdots & \vdots & \ddots & \vdots \\
a_{m}\times b_{1} & a_{m}\times b_{2} & \cdots & a_{m}\times b_{n}
\end{bmatrix}
$$

It's not too tricky to implement such a thing in Agda; the secret is to write
down the type and use the type-checker to help us ensure that we haven't lost a
case anywhere.

```agda
  open Data.Vec
    using (map)

  postulate
    _*_ : ğ”¸ â†’ ğ”¸ â†’ ğ”¸

  _âŠ—_ : Vec ğ”¸ m â†’ Vec ğ”¸ n â†’ Matrix m n
  []       âŠ— ys = []
  (x âˆ· xs) âŠ— ys = map (x *_) ys âˆ· xs âŠ— ys
```

Now that we have the outer product, we can implement matrix multiplication by
taking the outer product of each row/column pair and doing a matrix addition
with the multiplication of the rest of the matrix. Start with the type:

```agda
  _*â‚˜_ : Matrix m n â†’ Matrix n p â†’ Matrix m p
```

Recall that in the definition of matrix multiplication, the *columns* of the
first matrix get paired with the *rows* of the latter. Since our matrices are in
row-major order, our induction naturally will proceed on the second argument,
since that's where the rows are. If we're out of rows, the result is
conceptually zero, but that doesn't typecheck, so instead we use `0â‚˜` which is
the matrix analogue:

```agda
  x *â‚˜ [] = 0â‚˜
```

Otherwise, we must pair a column from `x` with the row we just pulled off. We
can use `left/rest` to get the column, and then proceed with our outer product
added to the resultant multiplication:

```agda
  x *â‚˜ (r âˆ· rs) =
    let c , cs = left/rest x
      in (c âŠ— r) +â‚˜ (cs *â‚˜ rs)
```

As it happens, this definition of `_*â‚˜_` *is* indeed correct, but it's rather
hard to convince ourselves of that, isn't it? Recall the definition we gave
earlier, where the $c_{i,j}$ element in the resultant matrix was given by the
formula:

$$
c_{i,j} = \sum_{k = 1}^{n} a_{i,k} \times b_{k, j}
$$

Our implementation instead gives us a recursive definition:

$$
a \times_m b = (a_{-, 1} \otimes b_{1, -}) +_m ((a_{-, 2\dots}) \times_m (b_{2\dots, -}))
$$

which uses nonstandard notation to suggest pulling a column off a matrix via
$a_{-, 1}$ and the rest of the matrix as $a_{-, 2\dots}$. We can convince
ourselves of the correctness here by noticing that the induction is actually on
`p`, which means the rows and the columns on which we're doing the outer product
remain of length `m` and `n` respectively. Thus, each outer product still
results in a matrix of size $m \times n$, of which we add up exactly `p` in
number. Thus, our definition here performs `p` matrix additions, while the
mathematical definition performs `p` scalar additions in each cell.

These two definitions are thus equivalent, but there is significantly more
algebraic manipulation necessary to use `_*â‚˜_` as written. Notice that if we
wanted to prove anything about it, we would first need to inline the definitions
of `left/rest`, `_âŠ—_`, and `_+â‚˜_`, each of which is annoyingly recursive and
none of which will Agda automatically compute for us. It's thus rather more work
than we'd like to do! In choosing the row-major order as our representation,
we've obscured the mathematics we're trying to prove. Not only do we need to
still do the original mathematics, we also need to juggle the weight of our
representation.


### Function Representation

Rather than go forward with the row-major representation, we will try again with
a different representation and see how all the same things roll-out. We note
that where things really went wrong was that rows and columns were citizens of
differing standing. It was easy to work with rows, but difficult to work with
columns. Of course, we could always try a column-major ordering instead, but
that would merely move the challenges.

Instead, we find ourselves looking for a representation which doesn't make any
distinctions between the two dimensions. Any sort of inductive definition is
sure to build up matrices from smaller matrices, which is likely to give rise to
the same issues. Let's thus turn our attention to a function representation:

```agda
module matrix-functions {ğ”¸ : Set} where
  open import Data.Nat
    using (â„•; zero; suc)
  open import Data.Fin
    using (Fin; zero; suc)

  Matrix : â„• â†’ â„• â†’ Set
  Matrix m n = (i : Fin m) â†’ (j : Fin n) â†’ ğ”¸
```

A matrix is thus parameterized by its dimensions, and is represented by a
function which takes those indices and gives you back an element of `ğ”¸`. Giving
names to the `Fin` arguments here isn't strictly necessary, but it helps Agda
give meaningful names to indices as we work with matrices.

We can implement the zero matrix trivially, by simply ignoring the indices:

```agda
  private variable
    m n p : â„•

  postulate 0# : ğ”¸

  0â‚˜ : Matrix m n
  0â‚˜ _ _ = 0#
```

Furthermore, we can now implement the identity matrix straightforwardly. In
symbols, the identity function is a square ($n \times n$) matrix whose cells are
given by:

$$
c_{i,j} =
\begin{cases}
  1 & i = j \\
  0 & \text{otherwise}
\end{cases}
$$

In Agda:

```agda
  open import Data.Bool
    using (Bool; true; false; if_then_else_)

  infix 3 _==_
  _==_ : Fin n â†’ Fin n â†’ Bool
  zero == zero = true
  zero == suc y = false
  suc x == zero = false
  suc x == suc y = x == y

  postulate 1# : ğ”¸

  1â‚˜ : Matrix m m
  1â‚˜ i j = if i == j then 1# else 0#
```

We can implement the summation operator by way of `sum`, which takes a function
out of `Fin n` and adds up every term:

```agda
  postulate
    _+_ : ğ”¸ â†’ ğ”¸ â†’ ğ”¸

  open import Function
    using (id; _âˆ˜_)

  sum : (Fin n â†’ ğ”¸) â†’ ğ”¸
  sum {zero} v = 0#
  sum {suc n} v = v zero + sum {n} (v âˆ˜ suc)
```

With all of these pieces under our belt, the definition of matrix multiplication
is now extremely simple, and mirrors its mathematical counterpart exactly:

```agda
  postulate
    _*_ : ğ”¸ â†’ ğ”¸ â†’ ğ”¸

  _*â‚˜_ : Matrix m n â†’ Matrix n p â†’ Matrix m p
  (a *â‚˜ b) i j = sum Î» k â†’ a i k * b k j
```

Implementing matrix addition is also exceptionally easy under our new scheme,
corresponding again exactly with the mathematical definition:

```agda
  _+â‚˜_ : Matrix m n â†’ Matrix m n â†’ Matrix m n
  (a +â‚˜ b) i j = a i j + b i j
```

With a little bit of machinery in order to express equality of matrices:

```agda
  open import Relation.Binary.PropositionalEquality

  infix 0 _â‰¡â‚˜_
  _â‰¡â‚˜_ : (a b : Matrix m n) â†’ Set
  a â‰¡â‚˜ b = âˆ€ i j â†’ a i j â‰¡ b i j
```

We can now prove `+â‚˜-identityË¡` again.

```agda
  postulate
    +-identityË¡ : âˆ€ x â†’ 0# + x â‰¡ x

  +â‚˜-identityË¡ : (a : Matrix m n) â†’ 0â‚˜ +â‚˜ a â‰¡â‚˜ a
  +â‚˜-identityË¡ a i j
    rewrite +-identityË¡ (a i j)
      = refl
```

Compare the simplicity of this proof to the previous one we wrote for the
row-major implementation:

```agda
--  +â‚˜-identityË¡ ([] âˆ· rs)
--    rewrite +â‚˜-identityË¡ rs
--      = refl
--  +â‚˜-identityË¡ ((c âˆ· cs) âˆ· rs)
--    rewrite +-identityË¡ c
--    rewrite +â‚˜-identityË¡ rs
--      = cong (Î» Ï† â†’ (c âˆ· Ï†) âˆ· rs) (lemma cs)
--    where
--      lemma
--          : âˆ€ {m} (cs : Vec ğ”¸ m)
--          â†’ zipWith _+_ (replicate 0#) cs â‰¡ cs
--      lemma [] = refl
--      lemma (c âˆ· cs)
--        rewrite +-identityË¡ c
--        rewrite lemma cs
--          = refl
```

Clearly we are onto something with our new representation. A problem which once
was hard is now much easier. Content with our new representation, we can explore
the question of what *is* a matrix, and why do practitioners care so much about
them.


### Matrices as Functions

The type of `_*â‚˜_ : Matrix m n â†’ Matrix n p â†’ Matrix m p` is somewhat suspicious
in its requirement that both matrices have a `n` dimension, in different
positions, which gets eliminated being pushed through the multiplication.
Compare this type against that of function composition, namely `_âˆ˜_ : (B â†’ C) â†’
(A â†’ B) â†’ (A â†’ C)`, which seems oddly similar: the functions both need a `B`
parameter, on opposite sides of the arrow, which gets eliminated in the result.

This is not a coincidence, because nothing is ever a coincidence. Whenever the
indices need to align, you should immediately think "function" (or at least
"morphism," as we will discuss in @sec:categorytheory.) But, if matrices
correspond to functions, exactly which functions are we talking about? The
indices give us a clue --- the input must be parameterized by exactly one of
`m`, `n`, and the output must be the other. In every day tasks, matrices are
usually multiplied against column vectors. For example, if we think about a
2-dimensional space with XY coordinates, the corresponds to a 90 degree
rotation clockwise:

$$
\begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix}
$$

We can thus apply this matrix to a particular coordinate, let's say $(5, 6)$, as
follows:

$$
\begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix}
\times
\begin{bmatrix}
5 \\
6
\end{bmatrix}
$$

Viewed in this light, the XY coordinate is the input, the rotation matrix is the
function, and the result of the multiplication is the output. Thus, it seems
natural to call the "width" of the matrix its input index. Let's define the type
of vectors as functions into our scalar:

```agda
  Vec : â„• â†’ Set
  Vec n = Fin n â†’ ğ”¸
```

Nothing goes particularly wrong if we were to use the standard `Data.Vec`
encoding instead, but this saves us some lemmas to more naturally turn vectors
into matrices and vice versa. Given vectors, we can lift them into column
matrices:

```agda
  column : Vec m â†’ Matrix m 1
  column v i _ = v i
```

which gives rise to a natural definition of the interpretation of a matrix as a
function from vectors to vectors:

```agda
  âŒŠ_âŒ‹ : Matrix m n â†’ Vec n â†’ Vec m
  âŒŠ M âŒ‹ v i = (M *â‚˜ column v) i zero
```

This is merely a convention; nothing prevents us from multiplying on the left
instead. In fact, we will prove this fact later (see `áµ€-*â‚˜-braid`.) For the time
being, we'd like to prove that function composition is indeed a specification
for `_*â‚˜_`. That is, we'd like to work our way towards a proof of `âŒŠ*â‚˜âŒ‹âŸ¶âŒŠâŒ‹âˆ˜âŒŠâŒ‹ :
(g : Matrix m n) â†’ (f : Matrix n p) â†’ âˆ€ v â†’ âŒŠ g *â‚˜ f âŒ‹ v â‰— (âŒŠ g âŒ‹ âˆ˜ âŒŠ f âŒ‹) v`.
It's a bit of a mouthful, but really what we're saying here is that the
interpretation of matrix multiplication is the composition of the matrices
interpreted as functions.

We will build our way towards this proof, but as a helper lemma, it will be
valuable to show the extensionality of sum---that is, if we can show the
equivalence of each term in the sum, we can thus show the two sums themselves
are equal. This function requires a little bit of induction on the `Fin`ite
numbers, but is a straightforward application of rewriting:

```agda
  sum-ext
      : {f g : Fin m â†’ ğ”¸}
      â†’ f â‰— g
      â†’ sum f â‰¡ sum g
  sum-ext {zero} x = refl
  sum-ext {suc m} same
    rewrite same zero
    rewrite sum-ext (same âˆ˜ suc)
      = refl
```

Our next lemma is to show that multiplication distributes over `sum`. This is a
straightforward application of the fact that multiplication distributes over
addition; only, we need to repeat the argument for every term in the sum. We
assume two non-controversial facts about `ğ”¸`:

```agda
  postulate
    *-zeroË¡ : âˆ€ x â†’ 0# * x â‰¡ 0#
    *-+-distribÊ³ : âˆ€ x y z â†’ (x + y) * z â‰¡ (x * z) + (y * z)
```

and then can show `*-sum-distribÊ³` in earnest:

```agda
  *-sum-distribÊ³
    : {f : Fin m â†’ ğ”¸}
    â†’ (k : ğ”¸)
    â†’ sum f * k â‰¡ sum (Î» i â†’ f i * k)
  *-sum-distribÊ³ {zero} k = *-zeroË¡ k
  *-sum-distribÊ³ {suc m} {f} k
    rewrite *-+-distribÊ³ (f zero) (sum (f âˆ˜ suc)) k
    rewrite *-sum-distribÊ³ {f = f âˆ˜ suc} k
      = refl

  sum-zero : sum {m} (Î» _ â†’ 0#) â‰¡ 0#
  sum-zero {zero} = refl
  sum-zero {suc m}
    rewrite sum-zero {m}
      = +-identityË¡ 0#
```

There are a few more facts to prove about sums before we can get to the meat of
our proof. But first, another reasonable assumption about `ğ”¸` --- namely that
it's multiplication is commutative:

```agda
  postulate
    *-comm : âˆ€ x y â†’ x * y â‰¡ y * x
```

and, for the sake of the reader's (and the author's) sanity, we will postulate
`â€¦algebraâ€¦` stating that the intermediary step is a tedious-but-straightforward
application of grade-school algebra:

```agda
    â€¦algebraâ€¦ : {â„“ : Level} {A : Set â„“} {x y : A} â†’ x â‰¡ y
```

Returning to our final two lemmas: first, we can show that the sum of two `sum`s
over the same bounds is the `sum` of the sum.

```agda
  +-sum-hom
    : (f g : Fin m â†’ ğ”¸)
    â†’ sum f + sum g â‰¡ sum (Î» i â†’ f i + g i)
```

The proof of this is rather verbose, but is just some shuffling of the addition
terms and a recursive call:

```agda
  +-sum-hom {zero} f g = +-identityË¡ 0#
  +-sum-hom {suc m} f g = begin
      (f zero + sum (f âˆ˜ suc)) + (g zero + sum (g âˆ˜ suc))
    â‰¡âŸ¨ â€¦algebraâ€¦ âŸ©
      (f zero + g zero) + (sum (f âˆ˜ suc) + sum (g âˆ˜ suc))
    â‰¡âŸ¨ cong ((f zero + g zero) +_)
            (+-sum-hom (f âˆ˜ suc) (g âˆ˜ suc)) âŸ©
      (f zero + g zero) + sum (Î» i â†’ f (suc i) + g (suc i))
    âˆ
    where open â‰¡-Reasoning
```

Our final necessary lemma before showing that matrix multiplication is a model
for function composition is to show that we can arbitrarily swap nested `sum`s,
so long as doing so doesn't introduce any scoping issues. The idea is that,
given some function `f : Fin m â†’ Fin n â†’ ğ”¸`, we can freely interchange nested
`sum`s which iterate over `m` and `n`. First, the type:

```agda
  postulate
    *-zeroÊ³ : âˆ€ x â†’ x * 0# â‰¡ 0#

  -- TODO(sandy): write some prose about this, pull from above, fix it
  sum-0 : sum {m} (Î» k â†’ 0#) â‰¡ 0#
  sum-0 = begin
    sum (Î» k â†’ 0#)       â‰¡âŸ¨ sym (sum-ext (Î» _ â†’ *-zeroË¡ 0#)) âŸ©
    sum (Î» k â†’ 0# * 0#)  â‰¡âŸ¨ sym (*-sum-distribÊ³ 0#) âŸ©
    sum (Î» k â†’ 0#) * 0#  â‰¡âŸ¨ *-zeroÊ³ _ âŸ©
    0#                   âˆ
    where open â‰¡-Reasoning
```

```agda
  sum-sum-distrib
      : (f : Fin m â†’ Fin n â†’ ğ”¸)
      â†’ sum (Î» j â†’ sum (Î» k â†’ f j k))
      â‰¡ sum (Î» k â†’ sum (Î» j â†’ f j k))
```

Take a moment to really understand what's going on in this type signature before
continuing. The only difference in the terms we'd like to show equivalence of is
which `sum` binds `j` and which binds `k`. We can proceed by induction on `m`,
which first requires us to show the sum of many 0 terms is itself zero:

```agda
  sum-sum-distrib {zero} {n} f = sym (sum-0)
```

The inductive case isn't particularly interesting, we just need to get
everything into the right shape that we can invoke `sum-sum-distrib`:

```agda
  sum-sum-distrib {suc m} {n} f =
    begin
      sum (Î» k â†’ f zero k) + sum (Î» j â†’ sum (Î» k â†’ f (suc j) k))
    â‰¡âŸ¨ cong (sum _ +_) (sum-sum-distrib (Î» j â†’ f (suc j))) âŸ©
      sum (Î» k â†’ f zero k) + sum (Î» k â†’ sum (Î» j â†’ f (suc j) k))
    â‰¡âŸ¨ +-sum-hom _ _ âŸ©
      sum (Î» k â†’ f zero k + sum (Î» j â†’ f (suc j) k))
    âˆ
    where open â‰¡-Reasoning
```

Finally we get to the meat of our goal: to show that the interpretation of
matrix multiplication is the composition of the interpretation of matrices as
functions. Start with the type:

```agda
  âŒŠ*â‚˜âŒ‹âŸ¶âŒŠâŒ‹âˆ˜âŒŠâŒ‹
    : (g : Matrix m n)
    â†’ (f : Matrix n p)
    â†’ (v : Fin p â†’ ğ”¸)
    â†’ âŒŠ g *â‚˜ f âŒ‹ v â‰— (âŒŠ g âŒ‹ âˆ˜ âŒŠ f âŒ‹) v
```

The proof mostly writes itself, given the lemmas we've already proven. Of
course, if you were working this out for yourself, you'd start with `âŒŠ*â‚˜âŒ‹âŸ¶âŒŠâŒ‹âˆ˜âŒŠâŒ‹`
and work backwards, determining which lemmas you need and proving them. This is
one flaw of presenting a book as a literate Agda document; it's hard to show
things in the order they happen "in real life."

```agda
  âŒŠ*â‚˜âŒ‹âŸ¶âŒŠâŒ‹âˆ˜âŒŠâŒ‹ g f v i = begin
      sum (Î» j â†’ sum (Î» k â†’ g i k * f k j) * v j)
    â‰¡âŸ¨ sum-ext (Î» j â†’ *-sum-distribÊ³ (v j))  âŸ©
      sum (Î» j â†’ sum (Î» k â†’ (g i k * f k j) * v j))
    â‰¡âŸ¨ sum-sum-distrib (Î» j k â†’ (g i k * f k j) * v j) âŸ©
      sum (Î» k â†’ sum (Î» j â†’ (g i k * f k j) * v j))
    â‰¡âŸ¨ â€¦algebraâ€¦ âŸ©
      sum (Î» k â†’ sum (Î» j â†’ (f k j * v j) * g i k))
    â‰¡âŸ¨ sym (sum-ext (Î» k â†’ *-sum-distribÊ³ (g i k))) âŸ©
      sum (Î» k â†’ sum (Î» j â†’ f k j * v j) * g i k)
    â‰¡âŸ¨ sum-ext (Î» k â†’ *-comm _ _) âŸ©
      sum (Î» k â†’ g i k * sum (Î» j â†’ f k j * v j))
    âˆ
    where open â‰¡-Reasoning
```

As a nice sanity check, we would like it if `âŒŠ 1â‚˜ âŒ‹` were the `id` function. So
let's show it!

```agda
  postulate
    *-identityË¡ : âˆ€ x â†’ 1# * x â‰¡ x
    +-identityÊ³ : âˆ€ x â†’ x + 0# â‰¡ x

  âŒŠ1â‚˜âŒ‹ : âˆ€ v â†’ âŒŠ 1â‚˜ {m} âŒ‹ v â‰— v
  âŒŠ1â‚˜âŒ‹ {suc m} v zero
    rewrite *-identityË¡ (v zero)
    rewrite sum-ext (Î» x â†’ *-zeroË¡ (v (suc x)))
    rewrite sum-0 {m}
    rewrite +-identityÊ³ (v zero)
      = refl
  âŒŠ1â‚˜âŒ‹ v (suc x)
    rewrite *-zeroË¡ (v zero)
    rewrite âŒŠ1â‚˜âŒ‹ (v âˆ˜ suc) x
      = +-identityË¡ _
```

Let's return now to the question of whether we made a bad choice when defining
our interpretation as multiplication on the right by a column vector. To
contrast, we can implement `âŒŠ_âŒ‹â€²`, which performs multiplication on the left
with a row vector:

```agda
  row : (Fin n â†’ ğ”¸) â†’ Matrix 1 n
  row v _ j = v j

  âŒŠ_âŒ‹â€² : Matrix m n â†’ Vec m â†’ Vec n
  âŒŠ M âŒ‹â€² v i = (row v *â‚˜ M) zero i
```

My claim is that we don't need `âŒŠ_âŒ‹â€²`; instead, we can simply use `âŒŠ_âŒ‹` with the
*transpose* of the matrix. The transpose swaps a matrix's width for its height,
and vice versa:

```agda
  infix 100 _áµ€
  _áµ€ : Matrix m n â†’ Matrix n m
  (M áµ€) i j = M j i
```

It's trivial now to show that `âŒŠ_âŒ‹'` is nothing more than `âŒŠ_âŒ‹ âˆ˜ _áµ€` --- that
is, the interpretation of the transpose of the original matrix! The proof
depends only on the commutativity of multiplication, which makes sense when you
think about what these two operations must be doing:

```agda
  âŒŠâŒ‹â€²-is-âŒŠáµ€âŒ‹
      : (a : Matrix m n)
      â†’ (v : Vec m)
      â†’ âŒŠ a âŒ‹â€² v â‰— âŒŠ a áµ€ âŒ‹ v
  âŒŠâŒ‹â€²-is-âŒŠáµ€âŒ‹ a v x = sum-ext Î» k â†’ *-comm _ _

  âŒŠgáµ€âˆ˜fáµ€âŒ‹-âŒŠfâˆ˜gâŒ‹áµ€
      : (g : Matrix n m)
     â†’  (f : Matrix p n)
      â†’ g áµ€ *â‚˜ f áµ€ â‰¡â‚˜ (f *â‚˜ g) áµ€
  âŒŠgáµ€âˆ˜fáµ€âŒ‹-âŒŠfâˆ˜gâŒ‹áµ€ g f i j = sum-ext Î» _ â†’ *-comm _ _
```

Because of `âŒŠâŒ‹â€²-is-âŒŠáµ€âŒ‹`, we are able to make the arbitrary decision to multiply
on the right without any loss of generalization. Anyone who thinks we've made
the wrong decision is welcome to transpose their matrix first.



So, what kind of functions are representable as matrices? As it happens, they
are precisely the *linear maps* --- that is, the two properties must hold:

```agda
  map : (ğ”¸ â†’ ğ”¸) â†’ Vec m â†’ Vec m
  map f v i = f (v i)

  zip : (ğ”¸ â†’ ğ”¸ â†’ ğ”¸) â†’ Vec m â†’ Vec m â†’ Vec m
  zip f vâ‚ vâ‚‚ i = f (vâ‚ i) (vâ‚‚ i)

  record LinearFunction (f : Vec m â†’ Vec n) : Set where
    constructor _âŠ¢_
    field
      additive
          : âˆ€ vâ‚ vâ‚‚
          â†’ f (zip _+_ vâ‚ vâ‚‚) â‰— zip _+_ (f vâ‚) (f vâ‚‚)
      homogeneity
          : âˆ€ v x
          â†’ f (map (x *_) v) â‰— map (x *_) (f v)
  open LinearFunction

  open import Data.Product
    using (Î£; projâ‚; projâ‚‚)

  âŒˆ_âŒ‰ : {f : Vec n â†’ Vec m} â†’ LinearFunction f â†’ Matrix m n
  âŒˆ_âŒ‰ {f = f} _ i j = f (1â‚˜ j) i

  postulate
    vec-ext : {f g : Vec m} â†’ (âˆ€ i â†’ f i â‰¡ g i) â†’ f â‰¡ g
    *-identityÊ³ : âˆ€ x â†’ x * 1# â‰¡ x
    matrix-ext : {f g : Matrix m n} â†’ f â‰¡â‚˜ g â†’ f â‰¡ g

--   _*áµ¥_ : ğ”¸ â†’ Vec m â†’ Vec m
--   a *áµ¥ v = map (a *_) v

--   basis-sum : (v : Vec m) â†’ Vec m
--   basis-sum v x = sum Î» { k â†’ (v k *áµ¥ 1â‚˜ k) x }

--   v-is-basis : (v : Vec m) â†’ basis-sum v â‰— v
--   v-is-basis {suc m} v zero
--     rewrite *-identityÊ³ (v zero)
--     rewrite sum-ext (Î» k â†’ *-zeroÊ³ (v (suc k)))
--     rewrite sum-zero {m}
--       = +-identityÊ³ (v zero)
--   v-is-basis v (suc x)
--     rewrite *-zeroÊ³ (v zero)
--     rewrite v-is-basis (v âˆ˜ suc) x
--     rewrite +-identityË¡ (v (suc x))
--       = refl


  raise : Vec m â†’ Vec (suc m)
  raise v zero = 0#
  raise v (suc i) = v i

  +-raise-hom
      : âˆ€ vâ‚ vâ‚‚ x
      â†’ raise {m} (Î» i â†’ vâ‚ i + vâ‚‚ i) x â‰¡ raise vâ‚ x + raise vâ‚‚ x
  +-raise-hom vâ‚ vâ‚‚ zero rewrite +-identityÊ³ 0# = refl
  +-raise-hom vâ‚ vâ‚‚ (suc x) = refl

  *-raise-hom
      : âˆ€ v x â†’ raise {m} (map (x *_) v) â‰— map (x *_) (raise v)
  *-raise-hom v x zero
    rewrite *-zeroÊ³ x = refl
  *-raise-hom v x (suc i) = refl

  linear-raise
      : {f : Vec (suc m) â†’ Vec n}
      â†’ LinearFunction f
      â†’ LinearFunction (Î» i j â†’ f (raise i) j)
  additive (linear-raise {f = f} (add âŠ¢ _)) vâ‚ vâ‚‚ x =
    begin
      f (raise (Î» i â†’ vâ‚ i + vâ‚‚ i)) x
    â‰¡âŸ¨ cong (Î» Ï† â†’ f Ï† x) (vec-ext (+-raise-hom vâ‚ vâ‚‚)) âŸ©
      f (Î» i â†’ raise vâ‚ i + raise vâ‚‚ i) x
    â‰¡âŸ¨ add _ _ x âŸ©
      f (raise vâ‚) x + f (raise vâ‚‚) x
    âˆ
    where open â‰¡-Reasoning
  homogeneity (linear-raise {f = f} (_ âŠ¢ hom)) v x i =
    begin
      f (raise (map (_*_ x) v)) i
    â‰¡âŸ¨ cong (Î» Ï† â†’ f Ï† i) (vec-ext (*-raise-hom _ _)) âŸ©
      f (map (_*_ x) (raise v)) i
    â‰¡âŸ¨ hom _ x i âŸ©
      map (_*_ x) (f (raise v)) i
    âˆ
    where open â‰¡-Reasoning

  lemma : âˆ€ (f : Vec (suc m) â†’ Vec n)
            (i : Fin n) (j : Fin m) â†’
          f (1â‚˜ (suc j)) i â‰¡ f (raise (1â‚˜ j)) i
  lemma f i x =
    cong (Î» Ï† â†’ f Ï† i)
      (vec-ext Î» { zero â†’ refl
                 ; (suc n) â†’ refl
                 })

  lemmaâ‚ : (v : Vec (suc m)) (i : Fin (suc m)) â†’
          ((v zero * (if zero == i then 1# else 0#)) +
            raise (Î» xâ‚ â†’ v (suc xâ‚)) i)
          â‰¡ v i
  lemmaâ‚ v zero
    rewrite *-identityÊ³ (v zero)
      = +-identityÊ³ (v zero)
  lemmaâ‚ v (suc i)
    rewrite *-zeroÊ³ (v zero)
      = +-identityË¡ (v (suc i))

  linear-to-matrix
      : {f : Vec m â†’ Vec n}
      â†’ (lf : LinearFunction f)
      â†’ âˆ€ v
      â†’ âŒŠ âŒˆ lf âŒ‰ âŒ‹ v â‰— f v
  linear-to-matrix {zero} {n} {f} (add âŠ¢ hom) v x = begin
    0#                    â‰¡âŸ¨ sym (*-zeroË¡ _) âŸ©
    0# * f v x            â‰¡âŸ¨ sym (hom v 0# x) âŸ©
    f (Î» i â†’ 0# * v i) x  â‰¡âŸ¨ cong (Î» Ï† â†’ f Ï† x) (vec-ext Î» ()) âŸ©
    f v x                 âˆ
    where open â‰¡-Reasoning
  linear-to-matrix {suc m} {n} {f} (add âŠ¢ hom) v x =
    begin
      âŒŠ âŒˆ add âŠ¢ hom âŒ‰ âŒ‹ v x
    â‰¡âŸ¨âŸ©
      ((Î» i j â†’ f (1â‚˜ j) i) *â‚˜ column v) x zero
    â‰¡âŸ¨âŸ©
      sum (Î» i â†’ f (1â‚˜ i) x * v i)
    â‰¡âŸ¨âŸ©
      (f (1â‚˜ zero) x * v zero) + sum (Î» i â†’ f (1â‚˜ (suc i)) x * v (suc i))
    â‰¡âŸ¨âŸ©
      (f (1â‚˜ zero) x * v zero) + (((Î» i j â†’ f (1â‚˜ (suc j)) i) *â‚˜ column (v âˆ˜ suc)) x zero)
    â‰¡âŸ¨âŸ©
      (f (1â‚˜ zero) x * v zero) + âŒŠ (Î» i j â†’ f (1â‚˜ (suc j)) i)âŒ‹ (v âˆ˜ suc) x
    â‰¡âŸ¨ cong (Î» Ï† â†’ (f (1â‚˜ zero) x * v zero) + âŒŠ Ï† âŒ‹ (v âˆ˜ suc) x) (matrix-ext (lemma f)) âŸ©
      (f (1â‚˜ zero) x * v zero) + âŒŠ âŒˆ linear-raise (add âŠ¢ hom) âŒ‰ âŒ‹ (v âˆ˜ suc) x
    â‰¡âŸ¨ cong (Î» Ï† â†’ (f (1â‚˜ zero) x * v zero) + Ï†) (linear-to-matrix (linear-raise (add âŠ¢ hom)) (v âˆ˜ suc) x)  âŸ©
      (f (1â‚˜ zero) x * v zero) + f (raise (v âˆ˜ suc)) x
    â‰¡âŸ¨ â€¦algebraâ€¦ âŸ©
      (v zero * f (1â‚˜ zero) x) + f (raise (v âˆ˜ suc)) x
    â‰¡âŸ¨ sym (cong (_+ f (raise (v âˆ˜ suc)) x) (hom _ _ _)) âŸ©
      f (map (_*_ (v zero)) (1â‚˜ zero)) x + f (raise (v âˆ˜ suc)) x
    â‰¡âŸ¨ sym (add _ _ x) âŸ©
      f (Î» i â†’ (v zero * (if zero == i then 1# else 0#)) + raise (Î» xâ‚ â†’ v (suc xâ‚)) i) x
    â‰¡âŸ¨ cong (Î» Ï† â†’ f Ï† x) (vec-ext (lemmaâ‚ v)) âŸ©
      f v x
    âˆ
    where open â‰¡-Reasoning

  âŒŠâŒ‹-linear : (M : Matrix m n) â†’ LinearFunction âŒŠ M âŒ‹
  additive (âŒŠâŒ‹-linear M) vâ‚ vâ‚‚ i = begin
    âŒŠ M âŒ‹ (zip _+_ vâ‚ vâ‚‚) i                      â‰¡âŸ¨âŸ©
    sum (Î» j â†’ M i j * (vâ‚ j + vâ‚‚ j))            â‰¡âŸ¨ â€¦algebraâ€¦ âŸ©
    sum (Î» j â†’ (M i j * vâ‚ j) + (M i j * vâ‚‚ j))  â‰¡âŸ¨ sym (+-sum-hom _ _) âŸ©
    âŒŠ M âŒ‹ vâ‚ i + âŒŠ M âŒ‹ vâ‚‚ i                      âˆ
    where open â‰¡-Reasoning
  homogeneity (âŒŠâŒ‹-linear M) v x i = begin
    âŒŠ M âŒ‹ (map (x *_) v) i         â‰¡âŸ¨âŸ©
    sum (Î» j â†’ M i j * (x * v j))  â‰¡âŸ¨ â€¦algebraâ€¦ âŸ©
    sum (Î» j â†’ (M i j * v j) * x)  â‰¡âŸ¨ sym (*-sum-distribÊ³ x) âŸ©
    sum (Î» j â†’ M i j * v j) * x    â‰¡âŸ¨ *-comm _ x âŸ©
    x * sum (Î» j â†’ M i j * v j)    â‰¡âŸ¨âŸ©
    map (x *_) (âŒŠ M âŒ‹ v) i         âˆ
    where open â‰¡-Reasoning

```

subsets


